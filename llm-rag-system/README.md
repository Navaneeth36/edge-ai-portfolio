# LLM-Powered RAG System

## Overview
Retrieval-Augmented Generation system using quantized LLMs for efficient edge deployment.

## Optimization Features
- Model quantization reducing VRAM from 16GB to 4GB
- 35% faster inference through pruning
- Containerized with REST API serving

## Tech Stack
- PyTorch, LangChain, Hugging Face
- Docker, FastAPI
- ONNX Runtime for optimization
